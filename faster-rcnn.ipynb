{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1462296,"sourceType":"datasetVersion","datasetId":857191}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# DATA UNDERSTANDING","metadata":{}},{"cell_type":"code","source":"import json\nimport pprint\n\nwith open('/kaggle/input/coco-2017-dataset/coco2017/annotations/instances_train2017.json') as train_json:\n    train_file = json.load(train_json)","metadata":{"execution":{"iopub.status.busy":"2024-07-12T12:26:24.995753Z","iopub.execute_input":"2024-07-12T12:26:24.996688Z","iopub.status.idle":"2024-07-12T12:26:46.865132Z","shell.execute_reply.started":"2024-07-12T12:26:24.996658Z","shell.execute_reply":"2024-07-12T12:26:46.864302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Total train images: {}'.format(len(train_file['images'])))\nprint('================================================')\nprint('Eg:')\npprint.pprint(train_file['images'][0])\n","metadata":{"execution":{"iopub.status.busy":"2024-07-12T12:26:46.866625Z","iopub.execute_input":"2024-07-12T12:26:46.866919Z","iopub.status.idle":"2024-07-12T12:26:46.873337Z","shell.execute_reply.started":"2024-07-12T12:26:46.866894Z","shell.execute_reply":"2024-07-12T12:26:46.872365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Total train annotations: {}'.format(len(train_file['annotations'])))\nprint('================================================')\nprint('Eg:')\npprint.pprint(train_file['annotations'][0])","metadata":{"execution":{"iopub.status.busy":"2024-07-12T12:26:46.874468Z","iopub.execute_input":"2024-07-12T12:26:46.874802Z","iopub.status.idle":"2024-07-12T12:26:46.886935Z","shell.execute_reply.started":"2024-07-12T12:26:46.874763Z","shell.execute_reply":"2024-07-12T12:26:46.885995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DATA CLEANUP","metadata":{}},{"cell_type":"code","source":"from pandas import DataFrame as DF","metadata":{"execution":{"iopub.status.busy":"2024-07-12T12:26:46.889525Z","iopub.execute_input":"2024-07-12T12:26:46.890220Z","iopub.status.idle":"2024-07-12T12:26:46.897850Z","shell.execute_reply.started":"2024-07-12T12:26:46.890194Z","shell.execute_reply":"2024-07-12T12:26:46.897099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"category_person_id = None\n\ncategories = train_file['categories']\nfor category in categories:\n    if category['name'] == 'person':\n        category_person_id = category['id']\n        break\n\nprint('ID Label Person: {}'.format(category_person_id))","metadata":{"execution":{"iopub.status.busy":"2024-07-12T12:26:46.898918Z","iopub.execute_input":"2024-07-12T12:26:46.899240Z","iopub.status.idle":"2024-07-12T12:26:46.909700Z","shell.execute_reply.started":"2024-07-12T12:26:46.899215Z","shell.execute_reply":"2024-07-12T12:26:46.908820Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# SETUP TRAIN'S DATA","metadata":{"execution":{"iopub.status.busy":"2024-07-12T12:26:46.910577Z","iopub.execute_input":"2024-07-12T12:26:46.910812Z","iopub.status.idle":"2024-07-12T12:26:46.921421Z","shell.execute_reply.started":"2024-07-12T12:26:46.910791Z","shell.execute_reply":"2024-07-12T12:26:46.920632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"people_train_annotations = []\nnot_people_train_annotations = {}\n\ntrain_annotations = train_file['annotations']\nfor annotation in train_annotations:\n    if annotation['category_id'] == category_person_id:\n        people_train_annotations.append(annotation)\n    else:\n        if annotation['image_id'] in not_people_train_annotations:\n            not_people_train_annotations[annotation['image_id']] += 1\n        else:\n            not_people_train_annotations[annotation['image_id']] = 1\n\nprint(\"People's annotations {}\".format(len(people_train_annotations)))\nprint(\"=====================\")\nprint(\"Eg: {}\".format(people_train_annotations[0]))\n\nprint(not_people_train_annotations)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-12T13:56:23.715487Z","iopub.execute_input":"2024-07-12T13:56:23.716143Z","iopub.status.idle":"2024-07-12T13:56:24.302768Z","shell.execute_reply.started":"2024-07-12T13:56:23.716110Z","shell.execute_reply":"2024-07-12T13:56:24.301740Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"people_train_image_ids = [annotation['image_id'] for annotation in people_train_annotations]\n\nprint(\"People's only images' id eg: {}\".format(people_train_image_ids[:5]))","metadata":{"execution":{"iopub.status.busy":"2024-07-12T13:20:35.600378Z","iopub.execute_input":"2024-07-12T13:20:35.601282Z","iopub.status.idle":"2024-07-12T13:20:35.656680Z","shell.execute_reply.started":"2024-07-12T13:20:35.601248Z","shell.execute_reply":"2024-07-12T13:20:35.655688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get images and combine it with annotations\n# Using anotation's image_id\n# With help from Pandas' Dataframe for performance reason\n\ntrain_images_df = DF(train_file['images'])\n# train_images_df\n\npeople_train_annotations_df = DF(people_train_annotations)\n# people_train_annotations_df\n\npeople_train_images = DF.merge(people_train_annotations_df, train_images_df, left_on=\"image_id\", right_on=\"id\")\n\npeople_train_df = DF(people_train_images, columns=['image_id', 'bbox', 'file_name', 'coco_url'])\npeople_train_df = people_train_df[people_train_df['image_id'].notna()]\npeople_train_df = people_train_df.fillna(\"\").apply(list)\npeople_train_df.sort_values(by=\"bbox\")","metadata":{"execution":{"iopub.status.busy":"2024-07-12T13:50:02.261957Z","iopub.execute_input":"2024-07-12T13:50:02.262574Z","iopub.status.idle":"2024-07-12T13:50:05.578976Z","shell.execute_reply.started":"2024-07-12T13:50:02.262540Z","shell.execute_reply":"2024-07-12T13:50:05.578045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DATASET PREPARATION","metadata":{}},{"cell_type":"code","source":"pip install pycocotools","metadata":{"execution":{"iopub.status.busy":"2024-07-12T12:26:50.159544Z","iopub.execute_input":"2024-07-12T12:26:50.159833Z","iopub.status.idle":"2024-07-12T12:27:02.486025Z","shell.execute_reply.started":"2024-07-12T12:26:50.159807Z","shell.execute_reply":"2024-07-12T12:27:02.484819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torchvision import tv_tensors\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport requests\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2024-07-12T12:27:02.487794Z","iopub.execute_input":"2024-07-12T12:27:02.488188Z","iopub.status.idle":"2024-07-12T12:27:02.494056Z","shell.execute_reply.started":"2024-07-12T12:27:02.488150Z","shell.execute_reply":"2024-07-12T12:27:02.493150Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class COCODataset(Dataset):\n    def __init__(self, data, transform=None, count=None):\n        limit = len(data)\n\n        if count != None:\n            limit = count\n        \n        self.transform = transform\n        self.data = data[:count]\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        image_df = self.data.iloc[idx]\n        image_id = image_df['image_id']\n\n        img = requests.get(image_df['coco_url'], stream=True).raw\n        img = Image.open(img).convert(\"RGB\")\n        \n        boxes = []\n        labels = []\n        \n        annotations = self.data.loc[self.data['image_id'] == image_id]\n        for ann in [annotations]:\n            for bbox in ann['bbox']:\n                x_min, y_min, width, height = bbox\n                boxes.append([x_min, y_min, x_min + width, y_min + height])\n#                 boxes.append([x_min, y_min, width, height])\n                labels.append(category_person_id)\n\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        labels = torch.as_tensor(labels, dtype=torch.int64)\n\n        img = tv_tensors.Image(img)\n        target = {\"boxes\": boxes, \"labels\": labels}\n\n        if self.transform:\n            img = self.transform(img)\n\n        return img, target","metadata":{"execution":{"iopub.status.busy":"2024-07-12T12:27:02.495519Z","iopub.execute_input":"2024-07-12T12:27:02.496253Z","iopub.status.idle":"2024-07-12T12:27:02.528833Z","shell.execute_reply.started":"2024-07-12T12:27:02.496218Z","shell.execute_reply":"2024-07-12T12:27:02.527953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision.transforms import v2 as T\n\ndef compose_transformation(train=True):\n    transforms = []\n    \n    transforms.append(T.ToDtype(torch.float, scale=True))\n    transforms.append(T.ToPureTensor())\n\n    if train:\n        transforms.append(T.RandomHorizontalFlip(0.5))\n    \n    return T.Compose(transforms)","metadata":{"execution":{"iopub.status.busy":"2024-07-12T12:27:02.530075Z","iopub.execute_input":"2024-07-12T12:27:02.530411Z","iopub.status.idle":"2024-07-12T12:27:02.542660Z","shell.execute_reply.started":"2024-07-12T12:27:02.530379Z","shell.execute_reply":"2024-07-12T12:27:02.541750Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/engine.py\")\nos.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/utils.py\")\nos.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_utils.py\")\nos.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_eval.py\")\nos.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/transforms.py\")\n\nimport utils","metadata":{"execution":{"iopub.status.busy":"2024-07-12T12:27:02.543815Z","iopub.execute_input":"2024-07-12T12:27:02.544403Z","iopub.status.idle":"2024-07-12T12:27:03.151192Z","shell.execute_reply.started":"2024-07-12T12:27:02.544371Z","shell.execute_reply":"2024-07-12T12:27:03.150191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = COCODataset(people_train_df.sort_values(by=\"image_id\")[:500], compose_transformation(True))\ntrain_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=4, collate_fn = utils.collate_fn)\n\nprint('Data Batch count: {}'.format(len(train_dataloader)))","metadata":{"execution":{"iopub.status.busy":"2024-07-12T12:27:03.152874Z","iopub.execute_input":"2024-07-12T12:27:03.153261Z","iopub.status.idle":"2024-07-12T12:27:03.361291Z","shell.execute_reply.started":"2024-07-12T12:27:03.153227Z","shell.execute_reply":"2024-07-12T12:27:03.360374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##### SKIPABLE ####\n## PREVIEW DATASET ##\n\nimg, targets = train_dataset[5]\n\n# Setup Image\nimg = T.ToPILImage()(img)\nplt.figure(figsize=(8, 8))\nplt.imshow(img)\n\n# Setup Bounding Box\nfor bbox in targets['boxes']:\n    print(bbox)\n    x_min, y_min, width, height = bbox\n    rect = plt.Rectangle((x_min, y_min), width, height, fill=False, color='red', linewidth=2)\n    plt.gca().add_patch(rect)\n    plt.text(x_min + 3, y_min - 6, \"Person\", color='white', fontsize=10, bbox=dict(facecolor='red'))\n\n# plt.title(f\"Image {i + 1}\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-12T12:27:03.362708Z","iopub.execute_input":"2024-07-12T12:27:03.363093Z","iopub.status.idle":"2024-07-12T12:27:04.043663Z","shell.execute_reply.started":"2024-07-12T12:27:03.363059Z","shell.execute_reply":"2024-07-12T12:27:04.042800Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# MODEL SETUP","metadata":{}},{"cell_type":"code","source":"import torchvision.models.detection as detection","metadata":{"execution":{"iopub.status.busy":"2024-07-12T12:27:04.044836Z","iopub.execute_input":"2024-07-12T12:27:04.045127Z","iopub.status.idle":"2024-07-12T12:27:04.049294Z","shell.execute_reply.started":"2024-07-12T12:27:04.045102Z","shell.execute_reply":"2024-07-12T12:27:04.048511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setup Model\n# min_size = 128\n# max_size = 256\n# image_mean = [0.485, 0.456, 0.406]\n# image_std = [0.229, 0.224, 0.225]\n# model = detection.fasterrcnn_resnet50_fpn(weights='DEFAULT', min_size = min_size, max_size = max_size, image_mean = image_mean, image_std = image_std)\nmodel = detection.fasterrcnn_resnet50_fpn()\n\n# Setup Backbone\nnum_classes = 2 # ['people', 'background']\nin_features = model.roi_heads.box_predictor.cls_score.in_features\nmodel.roi_heads.box_predictor = detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n\n# Setup Device\ntorch.cuda.empty_cache()\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nmodel.to(device)\n\n# Setup Optimizer\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n\n# Setup Learning Rate Scheduler\nscheduler = torch.optim.lr_scheduler.StepLR(\n    optimizer,\n    step_size=3,\n    gamma=0.1\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-12T12:27:04.050622Z","iopub.execute_input":"2024-07-12T12:27:04.050887Z","iopub.status.idle":"2024-07-12T12:27:04.881190Z","shell.execute_reply.started":"2024-07-12T12:27:04.050864Z","shell.execute_reply":"2024-07-12T12:27:04.880381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# MODEL TRAINING","metadata":{}},{"cell_type":"code","source":"import time\nfrom engine import train_one_epoch, evaluate","metadata":{"execution":{"iopub.status.busy":"2024-07-12T12:27:04.882424Z","iopub.execute_input":"2024-07-12T12:27:04.882809Z","iopub.status.idle":"2024-07-12T12:27:04.887642Z","shell.execute_reply.started":"2024-07-12T12:27:04.882775Z","shell.execute_reply":"2024-07-12T12:27:04.886673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs_count = 2\nbest_metric = 0\nbest_weight = None\n\nfor epoch in range(epochs_count):\n    result = train_one_epoch(model, optimizer, train_dataloader, device, epoch, print_freq=25)\n\n    loss = result.meters['loss'].global_avg\n    current_metric = -loss\n\n    if best_metric == 0:\n        best_metric = current_metric\n        best_weight = model.state_dict()\n        torch.save(best_weight, '/kaggle/working/best.pt')\n\n    if current_metric > best_metric:\n        best_metric = current_metric\n        best_weight = model.state_dict()\n        torch.save(best_weight, '/kaggle/working/best.pt')\n        print(f\"Saved best weights with metric:Â {best_metric}\")\n\n    print(\"Epoch loss: {}\".format(loss))\n    scheduler.step()","metadata":{"execution":{"iopub.status.busy":"2024-07-12T12:31:31.100497Z","iopub.execute_input":"2024-07-12T12:31:31.101158Z","iopub.status.idle":"2024-07-12T12:36:44.278341Z","shell.execute_reply.started":"2024-07-12T12:31:31.101116Z","shell.execute_reply":"2024-07-12T12:36:44.276820Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision.utils import draw_bounding_boxes\nfrom torchvision.io import read_image\n\nimage = read_image(\"/kaggle/input/coco-2017-dataset/coco2017/test2017/000000000212.jpg\")\ntransform = compose_transformation(train=False)\n\nmodel.eval()\nwith torch.no_grad():\n    x = transform(image)\n    x = x[:3, ...].to(device) # convert RGBA -> RGB and move to device\n    predictions = model([x, ])\n    pred = predictions[0]\n\n\nimage = (255.0 * (image - image.min()) / (image.max() - image.min())).to(torch.uint8)\nimage = image[:3, ...]\npred_labels = [f\"score: {score:.3f}\" for label, score in zip(pred[\"labels\"], pred[\"scores\"])]\npred_boxes = pred[\"boxes\"].long()\noutput_image = draw_bounding_boxes(image, pred_boxes, pred_labels, colors=\"red\")\n\nplt.figure(figsize=(12, 12))\nplt.imshow(output_image.permute(1, 2, 0))","metadata":{"execution":{"iopub.status.busy":"2024-07-12T13:08:53.191950Z","iopub.execute_input":"2024-07-12T13:08:53.192318Z","iopub.status.idle":"2024-07-12T13:08:54.070874Z","shell.execute_reply.started":"2024-07-12T13:08:53.192289Z","shell.execute_reply":"2024-07-12T13:08:54.069812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}